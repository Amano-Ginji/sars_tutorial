% v2-acmsmall-sample.tex, dated March 6 2012
% This is a sample file for ACM small trim journals
%
% Compilation using 'acmsmall.cls' - version 1.3 (March 2012), Aptara Inc.
% (c) 2010 Association for Computing Machinery (ACM)
%
% Questions/Suggestions/Feedback should be addressed to => "acmtexsupport@aptaracorp.com".
% Users can also go through the FAQs available on the journal's submission webpage.
%
% Steps to compile: latex, bibtex, latex latex
%
% For tracking purposes => this is v1.3 - March 2012

\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax
\usepackage{float}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{amssymb}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{tabularx,ragged2e,booktabs}
\usepackage{rotating}
\usepackage{tikz}
\usetikzlibrary{calc,positioning,backgrounds,arrows.meta}
\usepackage{forest}
\usepackage{multirow}
\usepackage{array}
\usepackage{pifont}
\usepackage[english]{babel}
\newcommand*\rot{\rotatebox{90}}
\newcommand*\OK{\ding{51}}

\newcolumntype{P}{>{\raggedright\arraybackslash}p}

%\usepackage{amsthm}
\usepackage{mathtools}
\newtheorem{mydef}{Definition}
\newcommand{\mydefautorefname}{Definition}

\addto\extrasenglish{%
  \renewcommand{\sectionautorefname}{Section}%
  \renewcommand{\subsectionautorefname}{Section}%
  \renewcommand{\subsubsectionautorefname}{Section}%
  \renewcommand{\algorithmautorefname}{Algorithm}
}

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}


% Document starts
\begin{document}

% Page heads
\markboth{}{Comparison of Evaluation Protocols for Sequential Recommender Systems }

% Title portion
\title{Comparison of Evaluation Protocols for\\ Sequential Recommender Systems }
\author{
MS. UMBERTO DI FABRIZIO
\affil{Politecnico di Milano}
Tutor: PhD MASSIMO QUADRANA
\affil{Politecnico di Milano}
Prof. PAOLO CREMONESI
\affil{Politecnico di Milano}
}
\begin{abstract}
Recommender Systems are used to predict the user appreciation for some future items. One trend is to exploit the recent history of a user actions in order to enhance the performances of the recommender. Such sequential recommender systems (SRS) can be evaluated using different protocols, namely set or sequential. To this day, there has been no evaluation of the result obtained using one protocol over the other. In this work the protocols are compared with respect to the ranking among six classes of SRS using both precision and recall metrics, moreover the sensitivity of each protocol in comparing the recommenders is discussed. The results show that, even though there is almost no difference in the results produced by the two protocols, the set one may be the favorable choice given his highest sensitivity and faster run time.  
\end{abstract}
 
\keywords{Sequential Recommendation, Evaluation protocols}

\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
	In this work several Sequential Recommender Systems (SRS) are evaluated accordingly to two evaluation procedures.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
%che cos'e SRS
Recommender Systems are used to predict the user appreciation for some future items. Traditionally RSs attempt to make such predictions based on item-similarity or user-similarity as well as hybrid techniques, yet most often the behavior of a user is strictly related to the last actions performed e.g. last songs listened, last items purchased, last apps opened. For this reason it seems reasonable to exploit the user profile/context i.e. the last actions performed by an user, in order to make more reliable and accurate predictions. In the literature there are several systems that try to exploit the recent history of an user's actions, namely Sequential Recommender Systems (SRS).\\There are several approaches to build a SRS (which will be discussed in the next section) as well as several protocols to assess the performances of such items and it is of undeniable importance the assessment of different SRS accordingly to the same protocol.
\\
The objective of this work is to implement several classes of sequential recommender systems in order to compare their performance adopting two evaluation protocols: set and sequential. In the next sections the extent to which the two protocols may produce different results when assessing multiple recommenders is analyzed.
\section{CLASSES OF SEQUENTIAL RECOMMENDER SYSTEMS}
The classes of SRS that are used in this work are taken from the state-of-the-art SRS:
\begin{enumerate}
	\item \textbf{Popularity based} recommender is non-personalized and doesn't exploit the recent history of a user, yet it has been included for comparison purposes. The popularity recommender always predicts the top N most popular items.
	\item \textbf{Frequent Pattern Mining}\cite{mobasher02sequential} recommenders incorporate history in the system by leveraging on the ability to mine frequent sequences in the dataset and in so doing they can learn which sequences of items are more frequent, thus they can recommend the most probable next-item. The algorithm implemented adapts the SPMF library\cite{fournier2016spmf} and a prefix tree structure to keep sequences in memory. Once a pattern in the prefix tree is matched the children of the last node in the matched path are recommended as next items.
	\item \textbf{Markov Chain} recommenders \cite{shani05mdp} build a state graph where each state is a sequence of length k and k is the order of the markov model. In the simplest approach there is an edge between two states if the dataset has at least an occurrence of length k+1 made by concatenating the sequence in the first state with the last item of the next state. This definition is augment as proposed in \cite{shani05mdp} by applying skipping, clustering and model mixing. Although a markov model represents a clear method to exploit the sequence's history, it is computationally expensive in terms of both memory and running time, for this reason it has been constrained to a first order model. 
	\item \textbf{Prod2Vec} recommenders\cite{grbovic15prod2vec} are based on the theory of n-grams developed by the NLP community which takes into account the context in which a word is found and creates a distributed representation of that word. Word2Vec models have been used virtually in all types of applications\cite{any2vec}, in the case of recommender systems items (=words) are projected into a vector space by taking into account their context (=the k items before and after the one considered in the sequence). Similarity among items is calculated by applying the cosine similarity among their distributed representations. In this case the gensim\cite{rehurek2010software} library has been adapted to produce recommendations.
	\item \textbf{Supervised} recommenders try to model the historical data in such a way that classical atemporal machine learning algorithms can be used to make recommendations. In this work the \textit{data expansion} approach suggested in \cite{zidmars01temporal} has been used, together with a decision tree in order to obtain the final SRS.
	\item \textbf{Factorizing Personalized Markov Chains} represent a hybrid approach exploiting both the power of Markov Chains and Factorization models which have been extensively and successfully used in the literature of recommender system. This class of SRS has been built based on the paper\cite{rendle10FPMC} and by adapting the code at \cite{fpmcLib}.
\end{enumerate}

\section{EXPERIMENT SET UP}
The dataset is composed of 10k unique items, 380k sessions and 28k users, collected from the LastFM website. In order to make this work feasible the number of unique items among the sequences has been reduced to 500, which has been chosen since the dataset size is manageable but the recommendation task is not trivial.
For each of the recommenders described in the previous section the two protocols of evaluation are executed for $k \in [1,5,10]$ where $k$ is the number of items in the user profile accordingly to the first-k approach: for each sequence only the first k elements are used to recommend the future elements. In the case of set evaluation the prediction list is compared with the ground-truth whilst for the sequence evaluation the first k element are used to make the first recommendation which is evaluated only w.r.t. the first element of the ground-truth,then the user profile length is increased by one and the items are recommended again, this process goes on until the ground-truth has length 0. In this way it is also useful to understand for which values of k the recommendations are more accurate.
Table~\ref{data} summarizes the statistics for the datasets adopted for each value of $k$.
For each recommender precision and recall are evaluated on a test set obtained by a random holdout split procedure 80/20, and the metrics are averaged on 5 runs.\\ The parameters used to run each recommender have been set by empirical evaluation, yet it is worth to notice that the scale of the metrics didn't vary significantly, appendix A provides the parameters choice.

\begin{table}[]
	\centering
	\caption{Dataset statistics}
	\label{data}
	\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{k} & \textbf{Train Size} & \textbf{Test Size} & \textbf{Average sequence length} \\ \hline
		1          & 102193              & 25549              & 4.65                             \\ \hline
		5          & 23973               & 5994               & 10.30                            \\ \hline
		10         & 6947                & 1737               & 17.34                            \\ \hline
	\end{tabular}
\end{table}

\section{RESULTS}
In this section the result of the comparison of the two protocols are presented and discussed.\\
For each $k$ (i.e. length of user profile) precision and recall are the metrics adopted to evaluate the performances of the recommenders.  
%grafo seq vs set, sia presicion che recall
Figures~\ref{img:precision_k_1}~\ref{img:precision_k_5}~\ref{img:precision_k_10} show the results for the precision metric for the considered values of $k$. In the graphs the x-axis represents the result for the set evaluation while the y-axis the result of the sequential evaluation. The decision to adopt such a visualization technique is twofold: firstly the more a classifier is far from the diagonal the more its performances vary between the two protocols, secondly it is quick to realize the effect of the different evaluation protocols infact any recommender that appears in the bottom right or the upper left has very different metric scores between the two protocols. By analyzing the figures it is evident that the set evaluation usually leads to higher scores for precision yet recall has the same magnitude for the two protocols, more importantly if the recommenders can be aligned in  a monotonic increasing order it means that the two protocols provide the same ranking when ordering the recommenders.
\begin{figure}[H]
	\centering			
	\includegraphics[width=0.8\textwidth]{img/precision_k_1.png}
	\caption{Precision scores for the set evaluation and the sequential evaluation, with $k=1$.}
	\label{img:precision_k_1}
\end{figure}
\begin{figure}[H]
	\centering			
	\includegraphics[width=0.8\textwidth]{img/precision_k_5.png}
	\caption{Precision scores for the set evaluation and the sequential evaluation, with $k=5$.}
	\label{img:precision_k_5}
\end{figure}
\begin{figure}[H]
	\centering			
	\includegraphics[width=0.8\textwidth]{img/precision_k_10.png}
	\caption{Precision scores for the set evaluation and the sequential evaluation, with $k=10$.}
	\label{img:precision_k_10}
\end{figure}

Since the interest is to verify the correct order of the ranking of the recommenders for both the evaluation protocols, the following measure has been adopted in order to evaluate the difference of the rankings produced by the evaluation protocols :
\[ r = \frac{\frac{1}{L} \sum_{i}(\frac{1}{d_i + 1}) - \frac{1}{L}} {1-\frac{1}{L}} \]
where $L$ is the number of recommenders and $d_i$ is the distance in terms of positions in the rankings between the two evaluation protocols, $r=1$ when there is perfect agreement in the rankings, $0$ when there's total disagreement. It is possible to see that for all the scores of precision~\ref{img:precision_k_1}~\ref{img:precision_k_5}~\ref{img:precision_k_10} and recall~\ref{img:recall_k_1}~\ref{img:recall_k_5}~\ref{img:recall_k_10} $r=1$ except for fig~\ref{img:precision_k_10} where $r=0.8$ because the Markov model is better than FPMC for the sequential evaluation but worst for the set evaluation.\\
Anyway comparing the rankings is not the only interest, infact it is also worth to compare the sensitivity of the two protocols, in other words one evaluation protocol may differentiate between recommenders much more that the other, for instance in fig~\ref{img:precision_k_5} the difference of precision among FPM and FPMC is $0.02$ for the set evaluation while $0.06$ for the sequential protocol.\\ It is important to evaluate the sensitivity of the two protocol because even though they may give the same ranking, one protocol may produce a much larger gap between two models than the other and this may cause misleading conclusions.\\
It is possible to capture the sensitivity given two recommenders $i,j$, by computing the absolute difference in the metric between $i,j$ using one protocol and subtracting the absolute difference of the metric between $i,j$ using the other protocol. In so doing if the metric score of the set evaluation among two recommenders differs of $0.02$ and the same metric using the sequence evaluation differs of $0.06$ it is possible to say that the sequential evaluation is more sensitive. This idea can be extended by adding up the sensitivity value among two recommenders for each pair of recommenders in the evaluation, leading to the formula:

\[sensitivity = \frac{\sum_{i,j} |set_i - set_j| - |sequential_i-sequential_j|}{2} \]
where $set_x$ is the metric score for recommender $x$ calculated using the set protocol whilst $sequential_x$ is the score calculated with the sequential protocol.
%division by two because the same score is calculate for i,j and j,i
Notice that the sensitivity value does not say anything about the ranking among the recommenders but addresses the magnitude of difference among them w.r.t. the metric used. Using together the $r$ value and $sensitivity$ score it is possible to say a) whether the two evaluation protocols produce the same ranking among the recommenders b) the scale of difference in the metric score among them.

\begin{figure}[H]
	\centering			
	\includegraphics[width=0.8\textwidth]{img/recall_k_1.png}
	\caption{Recall scores for the set evaluation and the sequential evaluation, with $k=1$.}
	\label{img:recall_k_1}
\end{figure}

\begin{figure}[H]
	\centering			
	\includegraphics[width=0.8\textwidth]{img/recall_k_5.png}
	\caption{Recall scores for the set evaluation and the sequential evaluation, with $k=5$.}
	\label{img:recall_k_5}
\end{figure}

\begin{figure}[H]
	\centering			
	\includegraphics[width=0.8\textwidth]{img/recall_k_10.png}
	\caption{Recall scores for the set evaluation and the sequential evaluation, with $k=10$.}
	\label{img:recall_k_10}
\end{figure}

The sensitivity for the precision score for each $k$ is $(0.9534,  0.9856,  0.618)$ meaning that the set evaluation usually gives much more difference in the evaluation of the recommenders than the sequential one. With respect to the recall instead the sensitivity for each $k$ is $(-0.1463, -0.1227, -0.0994)$ meaning that the magnitude of the scores is almost the same yet slightingly stronger for the sequential evaluation.

\section{CONCLUSION}
In this work two evaluation protocols (i.e. set and sequential) for SRS are compared with respect to the ranking produced among the recommenders using both precision and recall. Six classes of state-of-the-art SRS are implemented in order to perform such evaluation. The analysis takes into account several values of $k$ (i.e. length of user profile) and assesses the difference among the two evaluation protocols in terms of $r$ value and sensitivity. By analyzing the $r$ value it is possible to conclude that the rankings produced by the two protocols are the same except for one case where the $r$ value is a little lower than 1. The analysis using the sensitivity value highlights the fact that the precision metric calculated using the set protocol usually gives much bigger gaps between the SRS whilst for the recall is close to 0. The conclusion that can be drawn from this work is that when one has to choose between the two protocols, and the use of the recommender (i.e. application scope) does not enforce one choice over the other, the set evaluation is advised because it gives a clearer picture of the difference among recommenders performance, moreover it is much faster to evaluate than the sequential one.

\newpage
\section{APPENDIX}
\subsection{}
\begin{itemize}
	\item \textbf{Popularity}
		\begin{description}
		\item [Top n = 5] Number of items recommended.
	\end{description}
	\item \textbf{Frequent Pattern Mining}: 
		\begin{description}
			\item [Min context = 1] The minimum context used while applying the all-kth algorithm\cite{nakagawa03impact}.
			\item [Max context = 10] The maximum context for the all-kth algorithm.
			\item [Min support = 0.002] Minimum support for frequent sequence mining.
			\item [Min confidence = 0.1] Minimum confidence to recommend an item.
						\item [Top n = 5] Number of items recommended.
		\end{description}
	\item \textbf{Markov Chain}
		\begin{description}
				\item [From k = 1] Minimum order or the model.
				\item [To k = 11] Maximum order of the model.
							\item [Top n = 5] Number of items recommended.
		\end{description}
	\item \textbf{Prod2Vec} 
	\begin{description}
		\item [Min count = 10] Minimum occurrences of an item, in order to be considered.
		\item [Size = 300]  The dimensionality of the feature vectors..
		\item [Window = 5]  The maximum distance between the current and predicted item within a sequence.
		\item [Top n = 5] Number of items recommended.
	\end{description}
	\item \textbf{Supervised} 
	\begin{description}
		\item [History length = 1] Length of history to consider acconrding to the data expansion model.\cite{zidmars01temporal}
					\item [Top n = 5] Number of items recommended.
	\end{description}
	\item \textbf{Factorizing Personalized Markov Chains} 
		\begin{description}
			\item [Latent factors = 92] Number of latent factors.
			\item [Learning rate = 0.01] Learning rate
			\item [Regularization = 0.001] Regularization term
			\item [Epochs = 40] Number of epochs		
			\item [Negative samples = 10] Number of negative samples			
			\item [Top n = 5] Number of items recommended.
	\end{description}
\end{itemize}
%struttura progetto, scalabilita'
%comandi principali


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{ACM-Reference-Format-Journals}
\bibliography{bibliography}

\end{document}

