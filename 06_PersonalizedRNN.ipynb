{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "1. [Load the dataset](#load_the_dataset)\n",
    "2. [Split the dataset](#split_the_dataset)\n",
    "3. [Fitting the recommender](#fitting)\n",
    "4. [Sequential evaluation](#seq_evaluation)  \n",
    "    4.1 [Evaluation with sequentially revaeled user profiles](#eval_seq_rev)  \n",
    "    4.2 [Evaluation with \"static\" user profiles](#eval_static)  \n",
    "5. [Analysis of next-item recommendation](#next-item)  \n",
    "    5.1 [Evaluation with different recommendation list lengths](#next-item_list_length)  \n",
    "    5.2 [Evaluation with different user profile lengths](#next-item_profile_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.data_utils import create_seq_db_filter_top_k, sequences_to_spfm_format\n",
    "from util.split import random_holdout, temporal_holdout\n",
    "from util.metrics import precision, recall, mrr\n",
    "from util import evaluation\n",
    "from recommenders.RNNRecommender import RNNRecommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_sequences_and_users(test_data, given_k, train_users):\n",
    "    # we can run evaluation only over sequences longer than abs(LAST_K)\n",
    "    mask = test_data['sequence'].map(len) > abs(given_k)\n",
    "    mask &= test_data['user_id'].isin(train_users)\n",
    "    test_sequences = test_data.loc[mask, 'sequence'].values\n",
    "    test_users = test_data.loc[mask, 'user_id'].values\n",
    "    return test_sequences, test_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_the_dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load the dataset\n",
    "\n",
    "For this hands-on session we will use a dataset of user-listening sessions crawled from [last.fm](https://www.last.fm/). In detail, we will use a subset of the following dataset:\n",
    "\n",
    "* 30Music listening and playlists dataset, Turrin et al., ACM RecSys 2015 ([paper](https://home.deib.polimi.it/pagano/portfolio/papers/30Musiclisteningandplaylistsdataset.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mquadrana/workspace/sars_tutorial/util/data_utils.py:24: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n",
      "  aggregated = groups['item_id'].agg({'sequence': lambda x: list(map(str, x))})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = 'datasets/sessions.csv'\n",
    "\n",
    "# for the sake of speed, let's keep only the top-5k most popular items \n",
    "dataset = create_seq_db_filter_top_k(path=dataset_path,\n",
    "                                     topk=1000, \n",
    "                                     last_months=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see at how the dataset looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>ts</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>[1762, 3700, 638]</td>\n",
       "      <td>1420059172</td>\n",
       "      <td>2432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>[3772, 3953]</td>\n",
       "      <td>1419418147</td>\n",
       "      <td>15861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>[245, 1271, 379]</td>\n",
       "      <td>1419433841</td>\n",
       "      <td>15861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>[245, 1197, 4307, 3868]</td>\n",
       "      <td>1421674741</td>\n",
       "      <td>15861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>[409, 234, 2334, 2431, 231, 4738, 219, 2403]</td>\n",
       "      <td>1421679507</td>\n",
       "      <td>15861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sequence          ts  user_id\n",
       "session_id                                                                   \n",
       "122                                    [1762, 3700, 638]  1420059172     2432\n",
       "223                                         [3772, 3953]  1419418147    15861\n",
       "226                                     [245, 1271, 379]  1419433841    15861\n",
       "243                              [245, 1197, 4307, 3868]  1421674741    15861\n",
       "245         [409, 234, 2334, 2431, 231, 4738, 219, 2403]  1421679507    15861"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's show some statistics about the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter()\n",
    "dataset.sequence.map(cnt.update);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items: 1000\n",
      "Number of users: 17816\n",
      "Number of sessions: 65917\n",
      "\n",
      "Session length:\n",
      "\tAverage: 4.19\n",
      "\tMedian: 3.0\n",
      "\tMin: 1\n",
      "\tMax: 198\n",
      "Sessions per user:\n",
      "\tAverage: 3.70\n",
      "\tMedian: 3.0\n",
      "\tMin: 1\n",
      "\tMax: 38\n"
     ]
    }
   ],
   "source": [
    "sequence_length = dataset.sequence.map(len).values\n",
    "n_sessions_per_user = dataset.groupby('user_id').size()\n",
    "\n",
    "print('Number of items: {}'.format(len(cnt)))\n",
    "print('Number of users: {}'.format(dataset.user_id.nunique()))\n",
    "print('Number of sessions: {}'.format(len(dataset)) )\n",
    "\n",
    "print('\\nSession length:\\n\\tAverage: {:.2f}\\n\\tMedian: {}\\n\\tMin: {}\\n\\tMax: {}'.format(\n",
    "    sequence_length.mean(), \n",
    "    np.quantile(sequence_length, 0.5), \n",
    "    sequence_length.min(), \n",
    "    sequence_length.max()))\n",
    "\n",
    "print('Sessions per user:\\n\\tAverage: {:.2f}\\n\\tMedian: {}\\n\\tMin: {}\\n\\tMax: {}'.format(\n",
    "    n_sessions_per_user.mean(), \n",
    "    np.quantile(n_sessions_per_user, 0.5), \n",
    "    n_sessions_per_user.min(), \n",
    "    n_sessions_per_user.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most popular items: [('443', 1970), ('1065', 1526), ('67', 1462), ('1622', 1212), ('2308', 1211)]\n"
     ]
    }
   ],
   "source": [
    "print('Most popular items: {}'.format(cnt.most_common(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split_the_dataset'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, let's split the dataset randomly. NOTE: Sessions will be assigned either to the training or the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = random_holdout(dataset, perc=0.8, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 52733 - Test size: 13184\n"
     ]
    }
   ],
   "source": [
    "print(\"Train size: {} - Test size: {}\".format(len(train_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='fitting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fitting the recommender\n",
    "\n",
    "Here we fit the recommedation algorithm over the sessions in the training set.  \n",
    "\n",
    "This is a **simplified** interface to Recurrent Neural Network models for Session-based recommendation.\n",
    "Based on the following two papers:\n",
    "\n",
    "* Recurrent Neural Networks with Top-k Gains for Session-based Recommendations, Hidasi and Karatzoglou, CIKM 2018\n",
    "* Personalizing Session-based Recommendation with Hierarchical Recurrent Neural Networks, Quadrana et al, Recsys 2017\n",
    "\n",
    "In this notebook, we will consider the session-aware (**personalized**) version of the algorithm.  \n",
    "The hyper-parameters are:\n",
    "\n",
    "* `session_layers`: number of units per layer used at session level.\n",
    "    It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks.\n",
    "* `user_layers`: number of units per layer used at user level. Required only by personalized models.\n",
    "    It has to be a list of integers for multi-layer networks, or a integer value for single-layer networks.\n",
    "* `batch_size`: the mini-batch size used in training\n",
    "* `learning_rate`: the learning rate used in training (Adagrad optimized)\n",
    "* `momentum`: the momentum coefficient used in training\n",
    "* `dropout`: it's a 3-tuple with the values for the dropout of (user hidden, session hidden, user-to-session hidden) layers.\n",
    "* `epochs`: number of training epochs\n",
    "* `personalized`: whether to train a personalized model using the HRNN model (`True` in this case).\n",
    "\n",
    "**NOTE: HGRU4Rec originally has many more hyper-parameters, and checking them all is out from the scope of this tutorial. Check-out the original source code [here](https://github.com/mquad/hgru4rec).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-09-25 17:32:43,795 - INFO - Converting training data to GRU4Rec format\n",
      "2018-09-25 17:32:44,219 - INFO - Training started\n",
      "/Users/mquadrana/miniconda3/envs/srs/lib/python3.6/site-packages/theano/tensor/basic.py:6592: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  result[diagonal_slice] = x\n",
      "2018-09-25 17:33:23,066 - INFO - Epoch 0 - train cost: 0.9533\n",
      "2018-09-25 17:33:38,338 - INFO - Epoch 1 - train cost: 0.8611\n",
      "2018-09-25 17:33:52,569 - INFO - Epoch 2 - train cost: 0.8450\n",
      "2018-09-25 17:34:05,512 - INFO - Epoch 3 - train cost: 0.8398\n",
      "2018-09-25 17:34:18,010 - INFO - Epoch 4 - train cost: 0.8370\n",
      "2018-09-25 17:34:18,021 - INFO - Training completed\n"
     ]
    }
   ],
   "source": [
    "recommender = RNNRecommender(session_layers=[20], \n",
    "                             user_layers=[20],\n",
    "                             batch_size=16,\n",
    "                             learning_rate=0.5,\n",
    "                             momentum=0.1,\n",
    "                             dropout=(0.1,0.1,0.1),\n",
    "                             epochs=5,\n",
    "                             personalized=True)\n",
    "recommender.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='seq_evaluation'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sequential evaluation\n",
    "\n",
    "In the evaluation of sequence-aware recommenders, each sequence in the test set is split into:\n",
    "- the _user profile_, used to compute recommendations, is composed by the first *k* events in the sequence;\n",
    "- the _ground truth_, used for performance evaluation, is composed by the remainder of the sequence.\n",
    "\n",
    "In the cells below, you can control the dimension of the _user profile_ by assigning a **positive** value to `GIVEN_K`, which correspond to the number of events from the beginning of the sequence that will be assigned to the initial user profile. This ensures that each user profile in the test set will have exactly the same initial size, but the size of the ground truth will change for every sequence.\n",
    "\n",
    "Alternatively, by assigning a **negative** value to `GIVEN_K`, you will set the initial size of the _ground truth_. In this way the _ground truth_ will have the same size for all sequences, but the dimension of the user profile will differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "METRICS = {'precision':precision, \n",
    "           'recall':recall,\n",
    "           'mrr': mrr}\n",
    "TOPN = 10 # length of the recommendation list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval_seq_rev'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Evaluation with sequentially revealed user-profiles\n",
    "\n",
    "Here we evaluate the quality of the recommendations in a setting in which user profiles are revealed _sequentially_.\n",
    "\n",
    "The _user profile_ starts from the first `GIVEN_K` events (or, alternatively, from the last `-GIVEN_K` events if `GIVEN_K<0`).  \n",
    "The recommendations are evaluated against the next `LOOK_AHEAD` events (the _ground truth_).  \n",
    "The _user profile_ is next expanded to the next `STEP` events, the ground truth is scrolled forward accordingly, and the evaluation continues until the sequence ends.\n",
    "\n",
    "In typical **next-item recommendation**, we start with `GIVEN_K=1`, generate a set of **alternatives** that will evaluated against the next event in the sequence (`LOOK_AHEAD=1`), move forward of one step (`STEP=1`) and repeat until the sequence ends.\n",
    "\n",
    "You can set the `LOOK_AHEAD='all'` to see what happens if you had to recommend a **whole sequence** instead of a set of a set of alternatives to a user.\n",
    "\n",
    "NOTE: Metrics are averaged over each sequence first, then averaged over all test sequences.\n",
    "\n",
    "** (TODO) Try out with different evaluation settings to see how the recommandation quality changes. **\n",
    "\n",
    "\n",
    "![](gifs/sequential_eval.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIVEN_K=1, LOOK_AHEAD=1, STEP=1 corresponds to the classical next-item evaluation\n",
    "GIVEN_K = 1\n",
    "LOOK_AHEAD = 1\n",
    "STEP = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/8558 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8558 sequences available for evaluation (5903 users)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8558/8558 [00:51<00:00, 166.78it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sequences, test_users = get_test_sequences_and_users(test_data, GIVEN_K, train_data['user_id'].values) # we need user ids now!\n",
    "print('{} sequences available for evaluation ({} users)'.format(len(test_sequences), len(np.unique(test_users))))\n",
    "\n",
    "results = evaluation.sequential_evaluation(recommender,\n",
    "                                           test_sequences=test_sequences,\n",
    "                                           users=test_users,\n",
    "                                           given_k=GIVEN_K,\n",
    "                                           look_ahead=LOOK_AHEAD,\n",
    "                                           evaluation_functions=METRICS.values(),\n",
    "                                           top_n=TOPN,\n",
    "                                           scroll=False,\n",
    "                                           step=STEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential evaluation (GIVEN_K=1, LOOK_AHEAD=1, STEP=1)\n",
      "\tprecision@10: 0.0312\n",
      "\trecall@10: 0.3123\n",
      "\tmrr@10: 0.0899\n"
     ]
    }
   ],
   "source": [
    "print('Sequential evaluation (GIVEN_K={}, LOOK_AHEAD={}, STEP={})'.format(GIVEN_K, LOOK_AHEAD, STEP))\n",
    "for mname, mvalue in zip(METRICS.keys(), results):\n",
    "    print('\\t{}@{}: {:.4f}'.format(mname, TOPN, mvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='eval_static'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluation with \"static\" user-profiles\n",
    "\n",
    "Here we evaluate the quality of the recommendations in a setting in which user profiles are instead _static_.\n",
    "\n",
    "The _user profile_ starts from the first `GIVEN_K` events (or, alternatively, from the last `-GIVEN_K` events if `GIVEN_K<0`).  \n",
    "The recommendations are evaluated against the next `LOOK_AHEAD` events (the _ground truth_).  \n",
    "\n",
    "The user profile is *not extended* and the ground truth *doesn't move forward*.\n",
    "This allows to obtain \"snapshots\" of the recommendation performance for different user profile and ground truth lenghts.\n",
    "\n",
    "Also here you can set the `LOOK_AHEAD='all'` to see what happens if you had to recommend a **whole sequence** instead of a set of a set of alternatives to a user.\n",
    "\n",
    "**(TODO) Try out with different evaluation settings to see how the recommandation quality changes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIVEN_K = 1\n",
    "LOOK_AHEAD = 1\n",
    "STEP=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/9323 [00:00<00:42, 217.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9323 sequences available for evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9323/9323 [00:46<00:00, 201.74it/s]\n"
     ]
    }
   ],
   "source": [
    "test_sequences = get_test_sequences(test_data, GIVEN_K)\n",
    "print('{} sequences available for evaluation'.format(len(test_sequences)))\n",
    "\n",
    "results = evaluation.sequential_evaluation(recommender,\n",
    "                                           test_sequences=test_sequences,\n",
    "                                           given_k=GIVEN_K,\n",
    "                                           look_ahead=LOOK_AHEAD,\n",
    "                                           evaluation_functions=METRICS.values(),\n",
    "                                           top_n=TOPN,\n",
    "                                           scroll=False  # notice that scrolling is disabled!\n",
    "                                          )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential evaluation (GIVEN_K=1, LOOK_AHEAD=1, STEP=1)\n",
      "\tprecision@10: 0.0382\n",
      "\trecall@10: 0.3824\n",
      "\tmrr@10: 0.1034\n"
     ]
    }
   ],
   "source": [
    "print('Sequential evaluation (GIVEN_K={}, LOOK_AHEAD={}, STEP={})'.format(GIVEN_K, LOOK_AHEAD, STEP))\n",
    "for mname, mvalue in zip(METRICS.keys(), results):\n",
    "    print('\\t{}@{}: {:.4f}'.format(mname, TOPN, mvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='next-item'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analysis of next-item recommendation\n",
    "\n",
    "Here we propose to analyse the performance of the recommender system in the scenario of *next-item recommendation* over the following dimensions:\n",
    "\n",
    "* the *length* of the **recommendation list**, and\n",
    "* the *length* of the **user profile**.\n",
    "\n",
    "NOTE: This evaluation is by no means exhaustive, as different the hyper-parameters of the recommendation algorithm should be *carefully tuned* before drawing any conclusions. Unfortunately, given the time constraints for this tutorial, we had to leave hyper-parameter tuning out. A very useful reference about careful evaluation of (session-based) recommenders can be found at:\n",
    "\n",
    "*  Evaluation of Session-based Recommendation Algorithms, Ludewig and Jannach, 2018 ([paper](https://arxiv.org/abs/1803.09587))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='next-item_list_length'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Evaluation for different recommendation list lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "GIVEN_K = 1\n",
    "LOOK_AHEAD = 1\n",
    "STEP = 1\n",
    "topn_list = [1, 5, 10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9323 sequences available for evaluation\n"
     ]
    }
   ],
   "source": [
    "# ensure that all sequences have the same minimum length \n",
    "test_sequences = get_test_sequences(test_data, GIVEN_K)\n",
    "print('{} sequences available for evaluation'.format(len(test_sequences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/9323 [00:00<02:42, 57.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recommendation lists with length: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9323/9323 [03:54<00:00, 39.79it/s]\n",
      "  0%|          | 5/9323 [00:00<03:22, 46.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recommendation lists with length: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9323/9323 [03:35<00:00, 43.29it/s]\n",
      "  0%|          | 7/9323 [00:00<02:25, 64.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating recommendation lists with length: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 224/9323 [00:03<02:38, 57.24it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-5d16f7c52773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m                                                \u001b[0mtop_n\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                                \u001b[0mscroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# here we average over all profile lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                                                step=STEP)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mmvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMETRICS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mres_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/sars_tutorial/util/evaluation.py\u001b[0m in \u001b[0;36msequential_evaluation\u001b[0;34m(recommender, test_sequences, evaluation_functions, users, given_k, look_ahead, top_n, scroll, step)\u001b[0m\n\u001b[1;32m     47\u001b[0m                                                           \u001b[0mlook_ahead\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                                                           \u001b[0mtop_n\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                                                           step)\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 metrics += evaluate_sequence(recommender,\n",
      "\u001b[0;32m~/workspace/sars_tutorial/util/evaluation.py\u001b[0m in \u001b[0;36msequence_sequential_evaluation\u001b[0;34m(recommender, seq, evaluation_functions, user, given_k, look_ahead, top_n, step)\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0meval_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0meval_res\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mevaluate_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecommender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgiven_k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlook_ahead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0meval_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meval_res\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0meval_cnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/sars_tutorial/util/evaluation.py\u001b[0m in \u001b[0;36mevaluate_sequence\u001b[0;34m(recommender, seq, evaluation_functions, user, given_k, look_ahead, top_n)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_functions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecommender\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecommend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_profile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop_n\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/sars_tutorial/recommenders/RNNRecommender.py\u001b[0m in \u001b[0;36mrecommend\u001b[0;34m(self, user_profile, user_id)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0muser_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# sort items by predicted score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;31m# convert to the required output format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/srs/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36msort_values\u001b[0;34m(self, by, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[1;32m   4425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4426\u001b[0m             indexer = nargsort(k, kind=kind, ascending=ascending,\n\u001b[0;32m-> 4427\u001b[0;31m                                na_position=na_position)\n\u001b[0m\u001b[1;32m   4428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4429\u001b[0m         new_data = self._data.take(indexer,\n",
      "\u001b[0;32m~/miniconda3/envs/srs/lib/python3.6/site-packages/pandas/core/sorting.py\u001b[0m in \u001b[0;36mnargsort\u001b[0;34m(items, kind, ascending, na_position)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mnon_nans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_nans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mnon_nan_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_nan_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnon_nan_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnon_nans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res_list = []\n",
    "\n",
    "for topn in topn_list:\n",
    "    print('Evaluating recommendation lists with length: {}'.format(topn))\n",
    "    res_tmp = evaluation.sequential_evaluation(recommender,\n",
    "                                               test_sequences=test_sequences,\n",
    "                                               given_k=GIVEN_K,\n",
    "                                               look_ahead=LOOK_AHEAD,\n",
    "                                               evaluation_functions=METRICS.values(),\n",
    "                                               top_n=topn,\n",
    "                                               scroll=True,  # here we average over all profile lengths\n",
    "                                               step=STEP)\n",
    "    mvalues = list(zip(METRICS.keys(), res_tmp))\n",
    "    res_list.append((topn, mvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show separate plots per metric\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(METRICS), figsize=(15,5))\n",
    "res_list_t = list(zip(*res_list))\n",
    "for midx, metric in enumerate(METRICS):\n",
    "    mvalues = [res_list_t[1][j][midx][1] for j in range(len(res_list_t[1]))]\n",
    "    ax = axes[midx]\n",
    "    ax.plot(topn_list, mvalues)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xticks(topn_list)\n",
    "    ax.set_xlabel('List length')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='next-item_profile_length'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Evaluation for different user profile lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "given_k_list = [1, 2, 3, 4]\n",
    "LOOK_AHEAD = 1\n",
    "STEP = 1\n",
    "TOPN = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that all sequences have the same minimum length \n",
    "test_sequences = get_test_sequences(test_data, max(given_k_list))\n",
    "print('{} sequences available for evaluation'.format(len(test_sequences)))\n",
    "\n",
    "res_list = []\n",
    "\n",
    "for gk in given_k_list:\n",
    "    print('Evaluating profiles having length: {}'.format(gk))\n",
    "    res_tmp = evaluation.sequential_evaluation(recommender,\n",
    "                                               test_sequences=test_sequences,\n",
    "                                               given_k=gk,\n",
    "                                               look_ahead=LOOK_AHEAD,\n",
    "                                               evaluation_functions=METRICS.values(),\n",
    "                                               top_n=TOPN,\n",
    "                                               scroll=False,  # here we stop for each sequence length\n",
    "                                               step=STEP)\n",
    "    mvalues = list(zip(METRICS.keys(), res_tmp))\n",
    "    res_list.append((gk, mvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show separate plots per metric\n",
    "fig, axes = plt.subplots(nrows=1, ncols=len(METRICS), figsize=(15,5))\n",
    "res_list_t = list(zip(*res_list))\n",
    "for midx, metric in enumerate(METRICS):\n",
    "    mvalues = [res_list_t[1][j][midx][1] for j in range(len(res_list_t[1]))]\n",
    "    ax = axes[midx]\n",
    "    ax.plot(given_k_list, mvalues)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xticks(given_k_list)\n",
    "    ax.set_xlabel('Profile length')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srs",
   "language": "python",
   "name": "srs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
